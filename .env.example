# Anthropic API Configuration
# You can use either naming convention:

# Standard naming (preferred):
ANTHROPIC_API_KEY=your_api_key_here
ANTHROPIC_API_BASE=https://api.anthropic.com

# Alternative naming (also supported):
# ANTHROPIC_AUTH_TOKEN=your_api_key_here
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# Model configuration
# Available models (tested with API proxies):
# - claude-sonnet-4-5-20250929 (Claude 4.5 Sonnet - faster, recommended)
# - claude-opus-4-5-20251101 (Claude 4.5 Opus - more capable)
MODEL_NAME=claude-sonnet-4-5-20250929

# =============================================================================
# Output Token Configuration (NEW - prevents crashes on large outputs)
# =============================================================================

# Maximum output tokens per request (optional)
# Default: 160000
# Range: 1000 - 100000000 (Claude's limit)
#
# This controls how much the model can generate in a single response.
# Higher values allow longer outputs but use more API quota.
#
# Recommended values:
#   - 8000-16000: Normal tasks
#   - 32000-64000: Complex tasks with long outputs
#   - 160000+: Very large outputs (code generation, detailed analysis)
#
# MINI_CODE_MAX_OUTPUT_TOKENS=160000

# Maximum truncation retry attempts (optional)
# Default: 3
# Range: 1 - 10
#
# When output is truncated (hits max_tokens), the system will:
# 1. Ask the model to provide a summary instead
# 2. Retry up to this many times
# 3. If still truncated, fail with a helpful error message
#
# Recommended values:
#   - 2-3: Standard (balances recovery vs. API cost)
#   - 1: Fast fail (minimal retries)
#   - 5+: Persistent (more attempts to complete)
#
# MINI_CODE_MAX_TRUNCATION_RETRIES=3

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Standard configuration (default behavior)
# MINI_CODE_MAX_OUTPUT_TOKENS=160000
# MINI_CODE_MAX_TRUNCATION_RETRIES=3

# Example 2: High output tasks (large code generation, detailed reports)
# MINI_CODE_MAX_OUTPUT_TOKENS=640000
# MINI_CODE_MAX_TRUNCATION_RETRIES=2

# Example 3: Conservative (minimize API usage)
# MINI_CODE_MAX_OUTPUT_TOKENS=8000
# MINI_CODE_MAX_TRUNCATION_RETRIES=1

# Example 4: Maximum output (very large tasks)
# MINI_CODE_MAX_OUTPUT_TOKENS=100000000
# MINI_CODE_MAX_TRUNCATION_RETRIES=3

# =============================================================================
# How It Works
# =============================================================================

# The actual max_tokens used per request is dynamically calculated:
# 1. Estimate tokens used by conversation history (~4 chars = 1 token)
# 2. Calculate available space in Claude's 200K context window
# 3. Use 40% of remaining space for output
# 4. Limit by MINI_CODE_MAX_OUTPUT_TOKENS
# 5. Ensure minimum 4000 tokens
#
# When output is truncated:
# - Warning displayed: "Response truncated (attempt N/M)"
# - Model asked: "Provide brief summary or use write_file"
# - If retries exhausted: Error with hint to increase max_tokens or break task
#
# Applies to: v2_todo_agent, v3_subagent
